		###########################################################
		#Gathering and parsing SCLM logs with ELK stack + filebeat#
		###########################################################






Requirements 
Authentification with search quard 
Filebeat Configuration
Logstash Configuration 
Elastick Configuration 
Kibana Configuration 
Requirements
Version of all requirement services (except filebeat) must be suited with Version Matrix :



For versions 6.X  from (https://github.com/floragunncom/search-guard/wiki)

Elasticsearch Version 	Latest Search Guard Version	Latest Search Guard Kibana Version	Commercial support available
6.2.2 	6.2.2-21.0 	6.2.2-10	YES
6.2.1	6.2.1-21.0 	6.2.1-10	YES
6.1.3	6.1.3-21.0 	6.1.3-10	YES
6.1.2 	6.1.2-21.0	6.1.2-10	YES
6.1.1 	6.1.1-21.0	6.1.1-10	YES
6.1.0  	6.1.0-21.0 	6.1.0-10	YES


For versions 5.X  from (https://github.com/floragunncom/search-guard/wiki)

Elasticsearch Version  	Latest Search Guard Version	Commercial support available
5.6.8	5.6.8-19 	YES
5.6.7	5.6.7-19	YES
5.6.6	5.6.6-18	YES
5.6.5	5.6.5-18	YES
5.6.4	5.6.4-18	YES
5.6.3	5.6.3-18	YES
5.6.2	5.6.2-18	YES
5.6.0	5.6.0-18	YES
5.5.3

5.5.3-16	YES
5.5.2	5.5.2-16	YES
5.5.1	5.5.1-16	YES
5.5.0	5.5.0-16	YES
5.4.3	5.4.3-16 	YES
5.4.2	5.4.2-15 	YES
5.4.1	5.4.1-15	YES
5.4.0 	5.4.0-15	YES
5.3.3	5.3.3-15	YES
5.3.2	5.3.2-15	YES
5.3.1 	5.3.1-15	YES


Example project : Elasticksearch version   5.6.5 , Filebeat 6.2 



Download links:

Elasticsearch 5.6.5
Search-guard-5/5.6.5-18 for elasticsearch 5.6.5
search-guard-kibana-plugin - 5.6.5
logstash-5.6.5
kibana-5.6.5
filebeat- 6.2


Authentification with Search Guard
Available Kibana Welcome Page Link 










































Full Documentation for searchquard-5: docs.search-guard.com

All SG config files stored in /usr/share/elasticsearch/plugins/search-guard-5/sgconfig directory 

main configuration result for Search Guard :

searchguard: 
 dynamic: 
   kibana: 
   http: 
     anonymous_auth_enabled: false 
     xff: 
       enabled: false 
       internalProxies: '192\.168\.0\.10|192\.168\.0\.11' # regex pattern 
       remoteIpHeader:  'x-forwarded-for' 
       proxiesHeader:   'x-forwarded-by' 
   authc: 
     kerberos_auth_domain:  
       enabled: false 
       order: 6 
       http_authenticator: 
         type: kerberos # NOT FREE FOR COMMERCIAL USE 
         challenge: false 
         config: 
           krb_debug: false 
           strip_realm_from_principal: true 
       authentication_backend: 
         type: noop 
     basic_internal_auth_domain:  
       enabled: true 
       order: 4 
       http_authenticator: 
         type: basic 
         challenge: true 
       authentication_backend: 
         type: intern 
     proxy_auth_domain: 
       enabled: false 
       order: 3 
       http_authenticator: 
         type: proxy 
         challenge: false 
         config: 
           user_header: "x-proxy-user" 
           roles_header: "x-proxy-roles" 
       authentication_backend: 
         type: noop 
     host_auth_domain: 
       enabled: false 
       order: 1 
       http_authenticator: 
         type: host #DEPRECATED, will be removed in a future version 
         challenge: false 
       authentication_backend: 
         type: noop 
     jwt_auth_domain: 
       enabled: false 
       order: 0 
       http_authenticator: 
         type: jwt 
         challenge: true 
         config: 
           signing_key: "base64 encoded key" 
           jwt_header: "Authorization" 
           jwt_url_parameter: null 
           roles_key: null 
           subject_key: null 
       authentication_backend: 
         type: noop 
     clientcert_auth_domain: 
       enabled: false 
       order: 2 
       http_authenticator: 
         type: clientcert 
         config: 
           username_attribute: cn #optional, if omitted DN becomes username 
         challenge: false 
       authentication_backend: 
         type: noop 
     ldap: 
       enabled: true 
       order: 1 
       http_authenticator: 
         type: basic 
         challenge: false 
       authentication_backend: 
         type: ldap # NOT FREE FOR COMMERCIAL USE 
         config: 
           enable_ssl: false 
           enable_start_tls: false 
           enable_ssl_client_auth: false 
           verify_hostnames: true 
           hosts: 
             - LDAP server adress:389 
           bind_dn: CN=adreader,OU=Service Accounts,DC=determine,DC=org #cn=user with permisions to read LDAP , dc=determine, dc=org 
           password: LDAP user password 
           userbase: OU=Users,OU=Ukraine,DC=determine,DC=org # example ---------> 'ou=people,dc=example,dc=com' 
           usersearch: '(sAMAccountName={0})' 
           username_attribute: null 
   authz:   
     roles_from_myldap: 
       enabled: false 
       authorization_backend: 
         type: ldap # NOT FREE FOR COMMERCIAL USE 
         config: 
           enable_ssl: false 
           enable_start_tls: false 
           enable_ssl_client_auth: false 
           verify_hostnames: true 
           hosts: 
             - localhost:8389 
           bind_dn: null 
           password: null 
           rolebase: 'ou=groups,dc=example,dc=com' 
           rolesearch: '(member={0})' 
           userroleattribute: null 
           userrolename: disabled 
           rolename: cn 
           resolve_nested_roles: true 
           userbase: 'ou=people,dc=example,dc=com' 
           usersearch: '(uid={0})' 
     roles_from_another_ldap: 
       enabled: false 
       authorization_backend: 
         type: ldap # NOT FREE FOR COMMERCIAL USE




To reach  SCLM Aplication and catalina.out  logs from all Environments Filebeat sent the field: " customer " to  logstash via parsing stdout ""ps -aux | grep java" on dedicated hosts and creating a filebeat.prospectors. Logstash  create index via this field in ES . 



Adding permission for logstash to create needed indexes . 

sg_logstash: 
 cluster: 
   - indices:admin/template/get 
   - indices:admin/template/put 
   - CLUSTER_MONITOR 
   - CLUSTER_COMPOSITE_OPS 
 indices: 
   'logstash-*': 
     '*': 
       - CRUD 
       - CREATE_INDEX 
   '*beat*': 
     '*': 
       - CRUD 
       - CREATE_INDEX 
   'customer-name-1': 
     '*': 
       - CRUD 
       - CREATE_INDEX

.....
   'customer-name-n': 
     '*': 
        - CRUD 
       - CREATE_INDEX 



Adding Permissions for  Kibana Admin access :




sg_all_access: 
 users: 
   - admin 
   - "CN=LDAP(displayed_name),OU=Users,OU=Ukraine,DC=determine,DC=org" 
   Example: for opryadka@determine.org - "Oleg Pryadka" ; for olola@determine.org - "Oleg Lola"



To make changes on Search Guard Configuration runing  *sg_admin.sh with parametrs is required .
Filebeat Configuration 


Receive ({path to Environment logs directory} , {customer_name} ) via parsing stdout ""ps -aux | grep java".


- type : log 
enabled: true 
paths: 
- {path to Environment logs directory}/SCM.log 
- {path to Environment logs directory}/catalina.out 
fields: 
customer: {customer_name}
fields_under_root: true"



filebeat.config.modules:  
 path: ${path.config}/modules.d/*.yml  
 reload.enabled: false  
 setup.template.settings:  
 index.number_of_shards: 3  
 setup.kibana:  
 output.logstash:  
 hosts: ["10.100.0.54:{port number}"]





Logstash Configuration 
To start gathering logs with Filebeat need to create pipeline with 3 sections :




################################## input section ################################################ 
input { 
       beats { 
       type => "xerox-stage" 
       port => "5044" 
       codec => multiline { 
     pattern => "^\s" 
     what => "previous" 
       } 
   } 
}
########################### filters section #####################################################  need to change internal %timestamp to  %timestamp_from_log_message & create a custom field to json message with name ErrorType.

filter { 
       grok    { 
               match => { "message" => "%{TIMESTAMP_ISO8601:tstamp} %{LOGLEVEL:ErrorType }" }  
               } 
       date { 
               match => [ "tstamp", "ISO8601" ] 
               }
############################# output section ####################################################### 

output {


if [type] == "{customer_name}" 
{ 
elasticsearch { 
      hosts => [ "{elasticsearch host}" ] 
       index => [ "%{{customer_name}}" ] 
       user => logstash 
       password => logstash 
       ssl => true 
       ssl_certificate_verification => true 
       truststore => "/etc/elasticsearch/truststore.jks" 
       truststore_password => "changeit" 
  } 
} 
}





Example runnig Logstash from console with working example :  bin/logstash -f /etc/logstash/conf.d/example-pipeline.conf --config.reload.automatic



Kibana configuration 







